{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2PreTrainedModel\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from transformers import BertPreTrainedModel, BertModel, BertTokenizer,BertConfig\n",
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "from transformers import top_k_top_p_filtering\n",
    "from torch import nn\n",
    "from torch.nn import Identity\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class ValueHead(nn.Module):\n",
    "    \"\"\"The ValueHead class implements a head for GPT2 that returns a scalar for each output token.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.detach_head = False\n",
    "        self.summary_type = config.summary_type if hasattr(config, \"summary_type\") else \"last\"\n",
    "        if self.summary_type == \"attn\":\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.summary = Identity()\n",
    "        # if hasattr(config, \"summary_use_proj\") and config.summary_use_proj:\n",
    "        #     if hasattr(config, \"summary_proj_to_labels\") and config.summary_proj_to_labels and config.num_labels > 0:\n",
    "        #         num_classes = config.num_labels\n",
    "        #     else:\n",
    "        #         num_classes = config.hidden_size\n",
    "        num_classes = 1\n",
    "        self.summary = nn.Linear(config.hidden_size, num_classes)\n",
    "\n",
    "        self.activation = Identity()\n",
    "        if hasattr(config, \"summary_activation\") and config.summary_activation == \"tanh\":\n",
    "            self.activation = nn.Tanh()\n",
    "\n",
    "        self.first_dropout = Identity()\n",
    "        if hasattr(config, \"summary_first_dropout\") and config.summary_first_dropout > 0:\n",
    "            self.first_dropout = nn.Dropout(config.summary_first_dropout)\n",
    "\n",
    "        self.last_dropout = Identity()\n",
    "        if hasattr(config, \"summary_last_dropout\") and config.summary_last_dropout > 0:\n",
    "            self.last_dropout = nn.Dropout(config.summary_last_dropout)\n",
    "            \n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, hidden_states, cls_index=None):\n",
    "        if self.detach_head:\n",
    "            output = hidden_states.detach()\n",
    "        else:\n",
    "            output = hidden_states\n",
    "        output = self.first_dropout(output)\n",
    "        output = self.summary(output)\n",
    "        output = self.activation(output)\n",
    "        output = self.last_dropout(output)\n",
    "\n",
    "        return output\n",
    "def gelu(x):\n",
    "    \"\"\"Implementation of the gelu activation function.\n",
    "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
    "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "        Also see https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu}\n",
    "\n",
    "class BertPredictionHeadTransform(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.transform_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.transform_act_fn = config.hidden_act\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class BertLMPredictionHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.transform = BertPredictionHeadTransform(config)\n",
    "\n",
    "        # The output weights are the same as the input embeddings, but there is\n",
    "        # an output-only bias for each token.\n",
    "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
    "\n",
    "        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n",
    "        self.decoder.bias = self.bias\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class BertOnlyMLMHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.predictions = BertLMPredictionHead(config)\n",
    "\n",
    "    def forward(self, sequence_output):\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        return prediction_scores\n",
    "class BertLMHeadModel(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        if not config.is_decoder:\n",
    "            print(\"If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\")\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.cls = BertOnlyMLMHead(config)\n",
    "        self.v_head = ValueHead(config)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.cls.predictions.decoder\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.cls.predictions.decoder = new_embeddings\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        labels=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "            encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n",
    "                if the model is configured as a decoder.\n",
    "            encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used\n",
    "                in the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be\n",
    "                in `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100`\n",
    "                are ignored (masked), the loss is only computed for the tokens with labels n `[0, ...,\n",
    "                config.vocab_size]`\n",
    "            past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "                Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up\n",
    "                decoding.\n",
    "\n",
    "                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n",
    "                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n",
    "                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
    "            use_cache (`bool`, *optional*):\n",
    "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
    "                (see `past_key_values`).\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import BertTokenizer, BertLMHeadModel, BertConfig\n",
    "        >>> import torch\n",
    "\n",
    "        >>> tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "        >>> config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    "        >>> config.is_decoder = True\n",
    "        >>> model = BertLMHeadModel.from_pretrained(\"bert-base-cased\", config=config)\n",
    "\n",
    "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "        >>> outputs = model(**inputs)\n",
    "\n",
    "        >>> prediction_logits = outputs.logits\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        if labels is not None:\n",
    "            use_cache = False\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        prediction_scores = self.cls(sequence_output)\n",
    "\n",
    "        value = self.v_head(sequence_output).squeeze(-1)\n",
    "\n",
    "        outputs = (prediction_scores,) + outputs[1:] + (value,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "        \n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **model_kwargs):\n",
    "        input_shape = input_ids.shape\n",
    "        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n",
    "        if attention_mask is None:\n",
    "            attention_mask = input_ids.new_ones(input_shape)\n",
    "\n",
    "        # cut decoder_input_ids if past is used\n",
    "        if past is not None:\n",
    "            input_ids = input_ids[:, -1:]\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"past_key_values\": past}\n",
    "\n",
    "    def _reorder_cache(self, past, beam_idx):\n",
    "        reordered_past = ()\n",
    "        for layer_past in past:\n",
    "            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n",
    "        return reordered_past\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias', 'v_head.summary.bias', 'v_head.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "config.is_decoder = True\n",
    "model = BertLMHeadModel.from_pretrained(\"bert-base-uncased\", config=config)\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "logits,transformer_outputs,values = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 30522])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] --> and\n",
      "hello --> and\n",
      ", --> ,\n",
      "my --> and\n",
      "dog --> ,\n",
      "is --> ,\n",
      "cute --> ,\n",
      "[SEP] --> .\n"
     ]
    }
   ],
   "source": [
    "pred_ids = torch.argmax(logits, dim=-1)\n",
    "for i in range(inputs.input_ids.shape[1]):\n",
    "    current_id = tokenizer.decode(inputs.input_ids[:, i])\n",
    "    next_id = tokenizer.decode(pred_ids[:, i])\n",
    "    print(current_id, '-->', next_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond_to_batch(model, queries, txt_len=20, top_k=0, top_p=1.0):\n",
    "    \"\"\"Sample text from language model.\"\"\"\n",
    "    input_ids = queries\n",
    "    for i in range(txt_len):\n",
    "        # Get Logits\n",
    "        outputs = model(input_ids)\n",
    "        next_token_logits = outputs[0][:, -1, :]\n",
    "        next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "        # Sample\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        print(probs)\n",
    "        next_token = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1)\n",
    "    return input_ids[:, -txt_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([1, 7]), torch.Size([1, 7])]\n",
      "tensor([1012, 1012])\n",
      "tensor([1012, 1012])\n",
      "tensor([1012, 1012])\n",
      "tensor([1012, 1012])\n",
      "tensor([1012, 1012])\n",
      "tensor([1012, 1012])\n",
      "tensor([1012, 1012])\n",
      "tensor([1012, 1012])\n",
      "tensor([1012, 1012])\n",
      "tensor([1012, 1012])\n"
     ]
    }
   ],
   "source": [
    "query_txt_1 = \"My most favourite movie is\"\n",
    "query_txt_2 = \"My least favourite movie is\"\n",
    "queries_txt = [query_txt_1, query_txt_2]\n",
    "\n",
    "queries = [tokenizer.encode(query_txt, return_tensors=\"pt\") for query_txt in queries_txt]\n",
    "print([q.shape for q in queries])\n",
    "queries = torch.cat(queries)\n",
    "\n",
    "responses = respond_to_batch(model, queries, txt_len=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........\n",
      "My most favourite movie is..........\n",
      "..........\n",
      "My least favourite movie is..........\n"
     ]
    }
   ],
   "source": [
    "for i in range(responses.shape[0]):\n",
    "    response_txt = tokenizer.decode(responses[i])\n",
    "    query_txt = queries_txt[i]\n",
    "    print(query_txt + response_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8ed0073c41b023cdd22b6268c6b3f0c9b6d97a6234cf243afa672a36c79f6c6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
