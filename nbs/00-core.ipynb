{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions\n",
    "> A set of utility functions used throughout the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "def flatten_dict(nested, sep='/'):\n",
    "    \"\"\"Flatten dictionary and concatenate nested keys with separator.\"\"\"\n",
    "    def rec(nest, prefix, into):\n",
    "        for k, v in nest.items():\n",
    "            if sep in k:\n",
    "                raise ValueError(f\"separator '{sep}' not allowed to be in key '{k}'\")\n",
    "            if isinstance(v, collections.Mapping):\n",
    "                rec(v, prefix + k + sep, into)\n",
    "            else:\n",
    "                into[prefix + k] = v\n",
    "    flat = {}\n",
    "    rec(nested, '', flat)\n",
    "    return flat\n",
    "\n",
    "def stack_dicts(stats_dicts):\n",
    "    \"\"\"Stack the values of a dict.\"\"\"\n",
    "    results = dict()\n",
    "    for k in stats_dicts[0]:\n",
    "        stats_list = [torch.flatten(d[k]) for d in stats_dicts]\n",
    "        results[k] = torch.stack(stats_list)\n",
    "    return results\n",
    "\n",
    "def add_suffix(input_dict, suffix):\n",
    "    \"\"\"Add suffix to dict keys.\"\"\"\n",
    "    return dict((k + suffix, v) for k,v in input_dict.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "def pad_to_size(tensor, size, dim=1, padding=50256):\n",
    "    \"\"\"Pad tensor to size.\"\"\"\n",
    "    t_size = tensor.size()[dim]\n",
    "    if t_size==size:\n",
    "        return tensor\n",
    "    else:\n",
    "        return torch.nn.functional.pad(tensor, (0,size-t_size), 'constant', padding)\n",
    "\n",
    "def logprobs_from_logits(logits, labels):\n",
    "    \"\"\"\n",
    "    See: https://github.com/pytorch/pytorch/issues/563#issuecomment-330103591\n",
    "    \"\"\"\n",
    "    logp = F.log_softmax(logits, dim=2)\n",
    "    logpy = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logpy\n",
    "\n",
    "\n",
    "def whiten(values, shift_mean=True):\n",
    "    \"\"\"Whiten values.\"\"\"\n",
    "    mean, var = torch.mean(values), torch.var(values)\n",
    "    whitened = (values - mean) * torch.rsqrt(var + 1e-8)\n",
    "    if not shift_mean:\n",
    "        whitened += mean\n",
    "    return whitened\n",
    "\n",
    "def clip_by_value(x, tensor_min, tensor_max):\n",
    "    \"\"\"\n",
    "    Tensor extenstion to torch.clamp\n",
    "    https://github.com/pytorch/pytorch/issues/2793#issuecomment-428784713\n",
    "    \"\"\"\n",
    "    clipped = torch.max(torch.min(x, tensor_max), tensor_min)\n",
    "    return clipped\n",
    "\n",
    "def entropy_from_logits(logits):\n",
    "    \"\"\"Calculate entropy from logits.\"\"\"\n",
    "    pd = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    entropy = torch.logsumexp(logits, axis=-1) - torch.sum(pd*logits, axis=-1)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def average_torch_dicts(list_of_dicts):\n",
    "    \"\"\"Average values of a list of dicts wiht torch tensors.\"\"\"\n",
    "    average_dict = dict()\n",
    "    for key in list_of_dicts[0].keys():\n",
    "        average_dict[key] = torch.mean(torch.stack([d[key] for d in list_of_dicts]), axis=0)\n",
    "    return average_dict\n",
    "\n",
    "def stats_to_np(stats_dict):\n",
    "    \"\"\"Cast all torch.tensors in dict to numpy arrays.\"\"\"\n",
    "    new_dict = dict()\n",
    "    for k, v in stats_dict.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            new_dict[k] = v.detach().cpu().numpy()\n",
    "        else:\n",
    "            new_dict[k] = v\n",
    "        if np.isscalar(new_dict[k]):\n",
    "            new_dict[k] = float(new_dict[k])\n",
    "    return new_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "def build_bert_batch_from_txt(text_list, tokenizer, device):\n",
    "    \"\"\"Create token id and attention mask tensors from text list for BERT classification.\"\"\"\n",
    "    \n",
    "    # tokenize\n",
    "    tensors = [tokenizer.encode(txt, return_tensors=\"pt\").to(device) for txt in text_list]\n",
    "    \n",
    "    # find max length to pad to\n",
    "    max_len = max([t.size()[1] for t in tensors])\n",
    "    \n",
    "    # get padded tensors and attention masks\n",
    "    # (attention masks make bert ignore padding)\n",
    "    padded_tensors = []\n",
    "    attention_masks = []\n",
    "    for tensor in tensors:\n",
    "        attention_mask = torch.ones(tensor.size(), device=device)\n",
    "        padded_tensors.append(pad_to_size(tensor, max_len, padding=0))\n",
    "        attention_masks.append(pad_to_size(attention_mask, max_len, padding=0))\n",
    "    \n",
    "    # stack all tensors\n",
    "    padded_tensors = torch.cat(padded_tensors)\n",
    "    attention_masks = torch.cat(attention_masks)  \n",
    "    \n",
    "    return padded_tensors, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
