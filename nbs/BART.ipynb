{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForCausalLM,BartPretrainedModel\n",
    "from transformers.models.bart.modeling_bart import BartDecoderWrapper\n",
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import Identity\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ValueHead(nn.Module):\n",
    "    \"\"\"The ValueHead class implements a head for GPT2 that returns a scalar for each output token.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.detach_head = False\n",
    "        self.summary_type = config.summary_type if hasattr(config, \"summary_type\") else \"last\"\n",
    "        if self.summary_type == \"attn\":\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.summary = Identity()\n",
    "       \n",
    "        self.summary = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "        self.activation = Identity()\n",
    "        if hasattr(config, \"summary_activation\") and config.summary_activation == \"tanh\":\n",
    "            self.activation = nn.Tanh()\n",
    "\n",
    "        self.first_dropout = Identity()\n",
    "        if hasattr(config, \"summary_first_dropout\") and config.summary_first_dropout > 0:\n",
    "            self.first_dropout = nn.Dropout(config.summary_first_dropout)\n",
    "\n",
    "        self.last_dropout = Identity()\n",
    "        if hasattr(config, \"summary_last_dropout\") and config.summary_last_dropout > 0:\n",
    "            self.last_dropout = nn.Dropout(config.summary_last_dropout)\n",
    "            \n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, hidden_states, cls_index=None):\n",
    "        if self.detach_head:\n",
    "            output = hidden_states.detach()\n",
    "        else:\n",
    "            output = hidden_states\n",
    "        output = self.first_dropout(output)\n",
    "        output = self.summary(output)\n",
    "        output = self.activation(output)\n",
    "        output = self.last_dropout(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BartForCausalLM(BartPretrainedModel):\n",
    "    def __init__(self, config):\n",
    "        config.is_decoder = True\n",
    "        config.is_encoder_decoder = False\n",
    "        super().__init__(config)\n",
    "        self.model = BartDecoderWrapper(config)\n",
    "\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.v_head = ValueHead(config)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.decoder.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.model.decoder.embed_tokens = value\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def set_decoder(self, decoder):\n",
    "        self.model.decoder = decoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.model.decoder\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        head_mask=None,\n",
    "        cross_attn_head_mask=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
    "                provide it.\n",
    "\n",
    "                Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "                [`PreTrainedTokenizer.__call__`] for details.\n",
    "\n",
    "                [What are input IDs?](../glossary#input-ids)\n",
    "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n",
    "                if the model is configured as a decoder.\n",
    "            encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used\n",
    "                in the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n",
    "            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
    "                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 indicates the head is **not masked**,\n",
    "                - 0 indicates the head is **masked**.\n",
    "\n",
    "            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
    "                Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 indicates the head is **not masked**,\n",
    "                - 0 indicates the head is **masked**.\n",
    "\n",
    "            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
    "                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n",
    "                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n",
    "                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional\n",
    "                tensors are only required when the model is used as a decoder in a Sequence to Sequence model.\n",
    "\n",
    "                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n",
    "                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
    "\n",
    "                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n",
    "                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n",
    "                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "            use_cache (`bool`, *optional*):\n",
    "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
    "                (see `past_key_values`).\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (`bool`, *optional*):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (`bool`, *optional*):\n",
    "                Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import BartTokenizer, BartForCausalLM\n",
    "\n",
    "        >>> tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "        >>> model = BartForCausalLM.from_pretrained(\"facebook/bart-large\", add_cross_attention=False)\n",
    "        >>> assert model.config.is_decoder, f\"{model.__class__} has to be configured as a decoder.\"\n",
    "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "        >>> outputs = model(**inputs)\n",
    "\n",
    "        >>> logits = outputs.logits\n",
    "        ```\"\"\"\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        logits = self.lm_head(outputs[0])\n",
    "        hidden_states = outputs[0]\n",
    "        value = self.v_head(hidden_states).squeeze(-1)\n",
    "\n",
    "        outputs = (logits,) + outputs[1:] + (value,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, use_cache=None, **kwargs):\n",
    "        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n",
    "        if attention_mask is None:\n",
    "            attention_mask = input_ids.new_ones(input_ids.shape)\n",
    "\n",
    "        if past:\n",
    "            input_ids = input_ids[:, -1:]\n",
    "        # first step, decoder_cached_states are empty\n",
    "        return {\n",
    "            \"input_ids\": input_ids,  # encoder_outputs is defined. input_ids not needed\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"past_key_values\": past,\n",
    "            \"use_cache\": use_cache,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _reorder_cache(past, beam_idx):\n",
    "        reordered_past = ()\n",
    "        for layer_past in past:\n",
    "            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n",
    "        return reordered_past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /home/wzm289/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.da0f3c0e2dc1c2fecc46738a1ebf4806f2fc36aae3d5c1947f21e063e7cab34b\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at /home/wzm289/.cache/huggingface/transformers/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\n",
      "Some weights of the model checkpoint at facebook/bart-base were not used when initializing BartForCausalLM: ['encoder.layers.2.fc1.weight', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.4.fc2.bias', 'encoder.layers.2.fc2.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.fc2.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.3.fc1.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.1.fc2.weight', 'shared.weight', 'encoder.layers.2.fc2.weight', 'encoder.embed_tokens.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layernorm_embedding.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.4.fc1.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.embed_positions.weight', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.fc1.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.5.fc2.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.0.fc1.bias', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.4.fc2.weight', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.3.fc2.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layernorm_embedding.bias', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.5.fc1.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['v_head.summary.weight', 'v_head.summary.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading file https://huggingface.co/facebook/bart-base/resolve/main/vocab.json from cache at /home/wzm289/.cache/huggingface/transformers/43978bdeaa326572886b44fcfed82f932f76571095ce31973e51c3da8ccade7f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/facebook/bart-base/resolve/main/merges.txt from cache at /home/wzm289/.cache/huggingface/transformers/3c167ed8af56e6605eeb794b63a79d65d85e6708c9b04408d41946337030f5cd.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/facebook/bart-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/facebook/bart-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/facebook/bart-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading file https://huggingface.co/facebook/bart-base/resolve/main/tokenizer.json from cache at /home/wzm289/.cache/huggingface/transformers/a878fcd69bba037c9b1b227f4213579ae43d0aaa9374e167bc6c5f41b1cfeb30.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /home/wzm289/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.da0f3c0e2dc1c2fecc46738a1ebf4806f2fc36aae3d5c1947f21e063e7cab34b\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer\n",
    "model = BartForCausalLM.from_pretrained(\"facebook/bart-base\", add_cross_attention=False)\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "logits, transformer_outputs, values = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 50265])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ids = torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> -->  supplemented\n",
      "Hello --> ector\n",
      ", --> AMS\n",
      " my --> along\n",
      " dog -->  one\n",
      " is -->  believing\n",
      " cute -->  continued\n",
      "</s> --> and\n"
     ]
    }
   ],
   "source": [
    "for i in range(inputs.input_ids.shape[1]):\n",
    "    current_id = tokenizer.decode(inputs.input_ids[:, i])\n",
    "    next_id = tokenizer.decode(pred_ids[:, i])\n",
    "    print(current_id, '-->', next_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import top_k_top_p_filtering\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def respond_to_batch(model, queries, txt_len=20, top_k=0, top_p=1.0):\n",
    "    \"\"\"Sample text from language model.\"\"\"\n",
    "    input_ids = queries\n",
    "    for i in range(txt_len):\n",
    "        # Get Logits\n",
    "        outputs = model(input_ids)\n",
    "        next_token_logits = outputs[0][:, -1, :]\n",
    "        next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "        # Sample\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1)\n",
    "    return input_ids[:, -txt_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([1, 7]), torch.Size([1, 7])]\n",
      "torch.Size([2, 7, 768])\n",
      "torch.Size([2, 8, 768])\n",
      "torch.Size([2, 9, 768])\n",
      "torch.Size([2, 10, 768])\n",
      "torch.Size([2, 11, 768])\n",
      "torch.Size([2, 12, 768])\n",
      "torch.Size([2, 13, 768])\n",
      "torch.Size([2, 14, 768])\n",
      "torch.Size([2, 15, 768])\n",
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "query_txt_1 = \"My most favourite movie is\"\n",
    "query_txt_2 = \"My least favourite movie is\"\n",
    "queries_txt = [query_txt_1, query_txt_2]\n",
    "\n",
    "queries = [tokenizer.encode(query_txt, return_tensors=\"pt\") for query_txt in queries_txt]\n",
    "print([q.shape for q in queries])\n",
    "queries = torch.cat(queries)\n",
    "\n",
    "responses = respond_to_batch(model, queries, txt_len=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My most favourite movie is Buildingmac\"). ). lines shared shared\"). liberatingand\n",
      "My least favourite movie is SON combination combination combination combinationoperatedchannelchannelchannelchannel\n"
     ]
    }
   ],
   "source": [
    "for i in range(responses.shape[0]):\n",
    "    response_txt = tokenizer.decode(responses[i])\n",
    "    query_txt = queries_txt[i] + ' '\n",
    "    print(query_txt + response_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8ed0073c41b023cdd22b6268c6b3f0c9b6d97a6234cf243afa672a36c79f6c6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
